{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 医学論文の自動仕分けチャレンジ ベースライン\n",
    "\n",
    "## 背景・目的\n",
    "　近年、世界ではエビデンスに基づく医療（Evidence-Based Medicine; EBM）の実施が求められており、その根拠となる学術論文のシステマティック・レビュー（Systematic Review; SR）の重要性が高まっています。\n",
    "\n",
    "　システマティック・レビューとは、ライフサイエンス、特に医学分野において浸透している研究方法で、特定の研究テーマに対する文献をくまなく調査し、各研究データのバイアスを評価しながら、体系的に同質の研究データを収集・解析する研究手法のことを指します。\n",
    "\n",
    "　通常、システマティックレビューは以下のようなプロセスに沿って進められます。初期のステップとしては、網羅的かつ系統的に論文を検索・収集し、その中から対象の研究テーマに該当する論文を「選別」する作業が必要となります。また、様々な医学論文のデータベースを横断的に検索する上では、各データベースの特徴や機能を理解したうえで検索式を設定したり、あるいは既存の検索フィルタで選別が不十分な場合は人手による取捨選択を行うなど、多くの時間と労力が必要とされます。\n",
    "\n",
    "**▼一般的なSRの作業フロー**\n",
    "1. 研究テーマの決定\n",
    "2. 関連論文の検索、収集、選別\n",
    "3. 研究データの抽出、妥当性の評価\n",
    "4. データの要約\n",
    "5. （可能であれば）メタアナリシスによる統計学的解析\n",
    "6. 結果の解釈、結論\n",
    "\n",
    "そこで本コンペティションでは、システマティックレビューの効率化・省力化を目指し、網羅的に収集された論文の中から目的の論文を「選別」するための機械学習アルゴリズムの構築にチャレンジして頂きます。\n",
    "\n",
    "\n",
    "\n",
    "## タスク説明\n",
    "本コンペティションでは、論文のタイトルおよび抄録のテキストデータを用いて、システマティックレビューの対象となる文献か否か（2値）を判定するアルゴリズムの作成にチャレンジして頂きます。\n",
    "\n",
    "※コンペティションで使用するデータセットは、「診断精度研究」と呼ばれる学術分野において、当該分野の複数の研究者の方によって実施されたレビュー結果を元に作成しております。\n",
    "※データセットには、診断精度研究「以外」も含めた網羅的な文献データが含まれており、その中から診断精度研究に該当する文献（システマティックレビューの対象となる文献）を判定して頂くタスクとなります。\n",
    "※その他、データ形式などの詳細については、「データ」ページをご参照下さい。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## データに関する注意事項\n",
    "- 各論文データには、「文字化け」や「欠損値」が含まれる場合がございます。\n",
    "- 「タイトルは異なるが、実際には同一の論文である」といった論文のペアが含まれる可能性があります。本コンペティションでは、これらを別の論文であるとして評価を実施することとします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pytorch と BERT を使ったベースライン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers as T\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../dataset/\"\n",
    "OUTPUT_DIR = \"../output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 471\n",
    "seed_torch(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb  outputtrain.log\r\n"
     ]
    }
   ],
   "source": [
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "test = pd.read_csv(DATA_DIR + \"test.csv\")\n",
    "sub = pd.read_csv(DATA_DIR + \"sample_submit.csv\", header=None)\n",
    "sub.columns = [\"id\", \"judgement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023282372444280715\n"
     ]
    }
   ],
   "source": [
    "# この値を境に、モデルの出力を 0 と 1 にします。\n",
    "border = len(train[train[\"judgement\"] == 1]) / len(train[\"judgement\"])\n",
    "print(border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train):\n",
    "\n",
    "    # 交差検証 用の番号を振ります。\n",
    "    Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(train, train[\"judgement\"])):\n",
    "        train.loc[val_index, \"fold\"] = int(n)\n",
    "    train[\"fold\"] = train[\"fold\"].astype(np.uint8)\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(test):\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_train_data(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, df, model_name, include_labels=True):\n",
    "        tokenizer = T.BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.df = df\n",
    "        self.include_labels = include_labels\n",
    "\n",
    "        self.title = df[\"title\"].tolist()\n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.title,\n",
    "            padding = 'max_length',            \n",
    "            max_length = 72,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        if self.include_labels:\n",
    "            self.labels = df[\"judgement\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][idx])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][idx])\n",
    "\n",
    "        if self.include_labels:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "            return input_ids, attention_mask, label\n",
    "\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T.BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out = self.sigmoid(out.logits).squeeze()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0 or step == (len(train_loader) - 1):\n",
    "            print(\n",
    "                f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(valid_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # record score\n",
    "        preds.append(y_preds.to(\"cpu\").numpy())\n",
    "\n",
    "        if step % 100 == 0 or step == (len(valid_loader) - 1):\n",
    "            print(\n",
    "                f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    \"\"\"推論用関数\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    test_dataset = BaseDataset(test, \"bert-base-uncased\", include_labels=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True\n",
    "    )\n",
    "\n",
    "    for fold in range(5):\n",
    "        LOGGER.info(f\"========== model: bert-base-uncased fold: {fold} inference ==========\")\n",
    "        model = BaseModel(\"bert-base-uncased\")\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")[\"model\"])\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        for i, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(input_ids, attention_mask)\n",
    "            preds.append(y_preds.to(\"cpu\").numpy())\n",
    "        preds = np.concatenate(preds)\n",
    "        predictions.append(preds)\n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train, fold):\n",
    "    \"\"\"学習用関数\n",
    "    \"\"\"\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Data Loader\n",
    "    # ====================================================\n",
    "    trn_idx = train[train[\"fold\"] != fold].index\n",
    "    val_idx = train[train[\"fold\"] == fold].index\n",
    "\n",
    "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = BaseDataset(train_folds, \"bert-base-uncased\")\n",
    "    valid_dataset = BaseDataset(valid_folds, \"bert-base-uncased\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Model\n",
    "    # ====================================================\n",
    "    model = BaseModel(\"bert-base-uncased\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = T.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # ====================================================\n",
    "    # Loop\n",
    "    # ====================================================\n",
    "    best_score = -1\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(3):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        valid_labels = valid_folds[\"judgement\"].values\n",
    "\n",
    "        # scoring\n",
    "        score = fbeta_score(valid_labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n",
    "        )\n",
    "        LOGGER.info(f\"Epoch {epoch+1} - Score: {score}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f\"Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model\")\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\"\n",
    "            )\n",
    "\n",
    "    check_point = torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")\n",
    "\n",
    "    valid_folds[\"preds\"] = check_point[\"preds\"]\n",
    "\n",
    "    return valid_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result_df):\n",
    "    preds = result_df[\"preds\"].values\n",
    "    labels = result_df[\"judgement\"].values\n",
    "    score = fbeta_score(labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "    LOGGER.info(f\"Score: {score:<.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Training\n",
    "    oof_df = pd.DataFrame()\n",
    "    for fold in range(5):\n",
    "        _oof_df = train_loop(train, fold)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "        \n",
    "    # CV result\n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    get_result(oof_df)\n",
    "    \n",
    "    # Save OOF result\n",
    "    oof_df.to_csv(OUTPUT_DIR + \"oof_df.csv\", index=False)\n",
    "\n",
    "    # Inference\n",
    "    predictions = inference()\n",
    "    predictions = np.where(predictions < border, 0, 1)\n",
    "\n",
    "    # submission\n",
    "    sub[\"judgement\"] = predictions\n",
    "    sub.to_csv(OUTPUT_DIR + \"submission.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/1357] Elapsed 0m 19s (remain 448m 12s) Loss: 0.9016 \n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
