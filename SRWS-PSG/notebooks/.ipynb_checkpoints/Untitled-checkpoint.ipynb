{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 医学論文の自動仕分けチャレンジ ベースライン\n",
    "\n",
    "## 背景・目的\n",
    "　近年、世界ではエビデンスに基づく医療（Evidence-Based Medicine; EBM）の実施が求められており、その根拠となる学術論文のシステマティック・レビュー（Systematic Review; SR）の重要性が高まっています。\n",
    "\n",
    "　システマティック・レビューとは、ライフサイエンス、特に医学分野において浸透している研究方法で、特定の研究テーマに対する文献をくまなく調査し、各研究データのバイアスを評価しながら、体系的に同質の研究データを収集・解析する研究手法のことを指します。\n",
    "\n",
    "　通常、システマティックレビューは以下のようなプロセスに沿って進められます。初期のステップとしては、網羅的かつ系統的に論文を検索・収集し、その中から対象の研究テーマに該当する論文を「選別」する作業が必要となります。また、様々な医学論文のデータベースを横断的に検索する上では、各データベースの特徴や機能を理解したうえで検索式を設定したり、あるいは既存の検索フィルタで選別が不十分な場合は人手による取捨選択を行うなど、多くの時間と労力が必要とされます。\n",
    "\n",
    "**▼一般的なSRの作業フロー**\n",
    "1. 研究テーマの決定\n",
    "2. 関連論文の検索、収集、選別\n",
    "3. 研究データの抽出、妥当性の評価\n",
    "4. データの要約\n",
    "5. （可能であれば）メタアナリシスによる統計学的解析\n",
    "6. 結果の解釈、結論\n",
    "\n",
    "そこで本コンペティションでは、システマティックレビューの効率化・省力化を目指し、網羅的に収集された論文の中から目的の論文を「選別」するための機械学習アルゴリズムの構築にチャレンジして頂きます。\n",
    "\n",
    "\n",
    "\n",
    "## タスク説明\n",
    "本コンペティションでは、論文のタイトルおよび抄録のテキストデータを用いて、システマティックレビューの対象となる文献か否か（2値）を判定するアルゴリズムの作成にチャレンジして頂きます。\n",
    "\n",
    "※コンペティションで使用するデータセットは、「診断精度研究」と呼ばれる学術分野において、当該分野の複数の研究者の方によって実施されたレビュー結果を元に作成しております。\n",
    "※データセットには、診断精度研究「以外」も含めた網羅的な文献データが含まれており、その中から診断精度研究に該当する文献（システマティックレビューの対象となる文献）を判定して頂くタスクとなります。\n",
    "※その他、データ形式などの詳細については、「データ」ページをご参照下さい。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## データに関する注意事項\n",
    "- 各論文データには、「文字化け」や「欠損値」が含まれる場合がございます。\n",
    "- 「タイトルは異なるが、実際には同一の論文である」といった論文のペアが含まれる可能性があります。本コンペティションでは、これらを別の論文であるとして評価を実施することとします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pytorch と BERT を使ったベースライン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers as T\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../dataset/\"\n",
    "OUTPUT_DIR = \"../output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_logger(log_file=OUTPUT_DIR + \"train.log\"):\n",
    "    from logging import INFO, FileHandler, Formatter, StreamHandler, getLogger\n",
    "\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_torch(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 471\n",
    "seed_torch(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb  outputtrain.log\r\n"
     ]
    }
   ],
   "source": [
    "ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(DATA_DIR + \"train.csv\")\n",
    "test = pd.read_csv(DATA_DIR + \"test.csv\")\n",
    "sub = pd.read_csv(DATA_DIR + \"sample_submit.csv\", header=None)\n",
    "sub.columns = [\"id\", \"judgement\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.023282372444280715\n"
     ]
    }
   ],
   "source": [
    "# この値を境に、モデルの出力を 0 と 1 にします。\n",
    "border = len(train[train[\"judgement\"] == 1]) / len(train[\"judgement\"])\n",
    "print(border)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(train):\n",
    "\n",
    "    # 交差検証 用の番号を振ります。\n",
    "    Fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "    for n, (train_index, val_index) in enumerate(Fold.split(train, train[\"judgement\"])):\n",
    "        train.loc[val_index, \"fold\"] = int(n)\n",
    "    train[\"fold\"] = train[\"fold\"].astype(np.uint8)\n",
    "\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(test):\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = get_train_data(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseDataset(Dataset):\n",
    "    def __init__(self, df, model_name, include_labels=True):\n",
    "        tokenizer = T.BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        self.df = df\n",
    "        self.include_labels = include_labels\n",
    "\n",
    "        self.title = df[\"title\"].tolist()\n",
    "        self.encoded = tokenizer.batch_encode_plus(\n",
    "            self.title,\n",
    "            padding = 'max_length',            \n",
    "            max_length = 72,\n",
    "            truncation = True,\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        \n",
    "        if self.include_labels:\n",
    "            self.labels = df[\"judgement\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_ids = torch.tensor(self.encoded['input_ids'][idx])\n",
    "        attention_mask = torch.tensor(self.encoded['attention_mask'][idx])\n",
    "\n",
    "        if self.include_labels:\n",
    "            label = torch.tensor(self.labels[idx]).float()\n",
    "            return input_ids, attention_mask, label\n",
    "\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = T.BertForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        out = self.sigmoid(out.logits).squeeze()\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return \"%dm %ds\" % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return \"%s (remain %s)\" % (asMinutes(s), asMinutes(rs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(train_loader, model, criterion, optimizer, epoch, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "\n",
    "        # record loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 100 == 0 or step == (len(train_loader) - 1):\n",
    "            print(\n",
    "                f\"Epoch: [{epoch + 1}][{step}/{len(train_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(train_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    start = end = time.time()\n",
    "    losses = AverageMeter()\n",
    "\n",
    "    # switch to evaluation mode\n",
    "    model.eval()\n",
    "    preds = []\n",
    "\n",
    "    for step, (input_ids, attention_mask, labels) in enumerate(valid_loader):\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        batch_size = labels.size(0)\n",
    "\n",
    "        # compute loss\n",
    "        with torch.no_grad():\n",
    "            y_preds = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(y_preds, labels)\n",
    "        losses.update(loss.item(), batch_size)\n",
    "\n",
    "        # record score\n",
    "        preds.append(y_preds.to(\"cpu\").numpy())\n",
    "\n",
    "        if step % 100 == 0 or step == (len(valid_loader) - 1):\n",
    "            print(\n",
    "                f\"EVAL: [{step}/{len(valid_loader)}] \"\n",
    "                f\"Elapsed {timeSince(start, float(step + 1) / len(valid_loader)):s} \"\n",
    "                f\"Loss: {losses.avg:.4f} \"\n",
    "            )\n",
    "\n",
    "    predictions = np.concatenate(preds)\n",
    "    return losses.avg, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference():\n",
    "    \"\"\"推論用関数\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    test_dataset = BaseDataset(test, \"bert-base-uncased\", include_labels=False)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True\n",
    "    )\n",
    "\n",
    "    for fold in range(5):\n",
    "        LOGGER.info(f\"========== model: bert-base-uncased fold: {fold} inference ==========\")\n",
    "        model = BaseModel(\"bert-base-uncased\")\n",
    "        model.to(device)\n",
    "        model.load_state_dict(torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")[\"model\"])\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        for i, (input_ids, attention_mask) in tqdm(enumerate(test_loader), total=len(test_loader)):\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            with torch.no_grad():\n",
    "                y_preds = model(input_ids, attention_mask)\n",
    "            preds.append(y_preds.to(\"cpu\").numpy())\n",
    "        preds = np.concatenate(preds)\n",
    "        predictions.append(preds)\n",
    "    predictions = np.mean(predictions, axis=0)\n",
    "\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(train, fold):\n",
    "    \"\"\"学習用関数\n",
    "    \"\"\"\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Data Loader\n",
    "    # ====================================================\n",
    "    trn_idx = train[train[\"fold\"] != fold].index\n",
    "    val_idx = train[train[\"fold\"] == fold].index\n",
    "\n",
    "    train_folds = train.loc[trn_idx].reset_index(drop=True)\n",
    "    valid_folds = train.loc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    train_dataset = BaseDataset(train_folds, \"bert-base-uncased\")\n",
    "    valid_dataset = BaseDataset(valid_folds, \"bert-base-uncased\")\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=True,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        num_workers=4,\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Model\n",
    "    # ====================================================\n",
    "    model = BaseModel(\"bert-base-uncased\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = T.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # ====================================================\n",
    "    # Loop\n",
    "    # ====================================================\n",
    "    best_score = -1\n",
    "    best_loss = np.inf\n",
    "\n",
    "    for epoch in range(3):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # train\n",
    "        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, device)\n",
    "\n",
    "        # eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        valid_labels = valid_folds[\"judgement\"].values\n",
    "\n",
    "        # scoring\n",
    "        score = fbeta_score(valid_labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        LOGGER.info(\n",
    "            f\"Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s\"\n",
    "        )\n",
    "        LOGGER.info(f\"Epoch {epoch+1} - Score: {score}\")\n",
    "\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f\"Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model\")\n",
    "            torch.save(\n",
    "                {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\"\n",
    "            )\n",
    "\n",
    "    check_point = torch.load(OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\")\n",
    "\n",
    "    valid_folds[\"preds\"] = check_point[\"preds\"]\n",
    "\n",
    "    return valid_folds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result_df):\n",
    "    preds = result_df[\"preds\"].values\n",
    "    labels = result_df[\"judgement\"].values\n",
    "    score = fbeta_score(labels, np.where(preds < border, 0, 1), beta=7.0)\n",
    "    LOGGER.info(f\"Score: {score:<.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Training\n",
    "    oof_df = pd.DataFrame()\n",
    "    for fold in range(5):\n",
    "        _oof_df = train_loop(train, fold)\n",
    "        oof_df = pd.concat([oof_df, _oof_df])\n",
    "        LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "        get_result(_oof_df)\n",
    "        \n",
    "    # CV result\n",
    "    LOGGER.info(f\"========== CV ==========\")\n",
    "    get_result(oof_df)\n",
    "    \n",
    "    # Save OOF result\n",
    "    oof_df.to_csv(OUTPUT_DIR + \"oof_df.csv\", index=False)\n",
    "\n",
    "    # Inference\n",
    "    predictions = inference()\n",
    "    predictions = np.where(predictions < border, 0, 1)\n",
    "\n",
    "    # submission\n",
    "    sub[\"judgement\"] = predictions\n",
    "    sub.to_csv(OUTPUT_DIR + \"submission.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a92bf6ed0eb4b5a992d2d527405ead5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "torch>=1.5.0 is required for a normal functioning of this module, but found torch==1.4.0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-972361fa1b80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-47108ff0346a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0moof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfold\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0m_oof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0moof_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moof_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_oof_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"========== fold: {fold} result ==========\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-70cf408562d2>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(train, fold)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data_analysis/lib/python3.8/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, params, lr, betas, eps, weight_decay, correct_bias)\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mcorrect_bias\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     ):\n\u001b[0;32m--> 300\u001b[0;31m         \u001b[0mrequire_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch>=1.5.0\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# add_ with alpha\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid learning rate: {lr} - should be >= 0.0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data_analysis/lib/python3.8/site-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36mrequire_version\u001b[0;34m(requirement, hint)\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwant_ver\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant_ver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwanted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m             \u001b[0m_compare_versions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgot_ver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwant_ver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequirement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpkg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/data_analysis/lib/python3.8/site-packages/transformers/utils/versions.py\u001b[0m in \u001b[0;36m_compare_versions\u001b[0;34m(op, got_ver, want_ver, requirement, pkg, hint)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"want_ver is None\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgot_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwant_ver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         raise ImportError(\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;34mf\"{requirement} is required for a normal functioning of this module, but found {pkg}=={got_ver}.{hint}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         )\n",
      "\u001b[0;31mImportError\u001b[0m: torch>=1.5.0 is required for a normal functioning of this module, but found torch==1.4.0."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/kimoton/miniconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pytorch\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    agate-1.6.1                |     pyhd8ed1ab_0          54 KB  conda-forge\n",
      "    agate-dbf-0.2.2            |     pyhd8ed1ab_0           7 KB  conda-forge\n",
      "    agate-sql-0.5.7            |     pyhd8ed1ab_0          10 KB  conda-forge\n",
      "    babel-2.9.1                |     pyh44b312d_0         6.2 MB  conda-forge\n",
      "    cffi-1.14.6                |   py38ha65f79e_0         226 KB  conda-forge\n",
      "    expat-2.4.1                |       h9c3ff4c_0         182 KB  conda-forge\n",
      "    ffmpeg-4.3.2               |       hca11adc_0        92.0 MB  conda-forge\n",
      "    glib-2.68.4                |       h9c3ff4c_0         447 KB  conda-forge\n",
      "    glib-tools-2.68.4          |       h9c3ff4c_0          86 KB  conda-forge\n",
      "    greenlet-1.1.1             |   py38h709712a_0          83 KB  conda-forge\n",
      "    gstreamer-1.18.4           |       h76c114f_2         2.0 MB  conda-forge\n",
      "    jbig-2.1                   |    h7f98852_2003          43 KB  conda-forge\n",
      "    kiwisolver-1.3.2           |   py38h1fd1430_0          79 KB  conda-forge\n",
      "    ld_impl_linux-64-2.36.1    |       hea4e1c9_2         667 KB  conda-forge\n",
      "    lerc-2.2.1                 |       h9c3ff4c_0         213 KB  conda-forge\n",
      "    libclang-11.1.0            |default_ha53f305_1        19.2 MB  conda-forge\n",
      "    libdeflate-1.7             |       h7f98852_5          67 KB  conda-forge\n",
      "    libgcc-ng-11.1.0           |       hc902ee8_8         908 KB  conda-forge\n",
      "    libgfortran-ng-11.1.0      |       h69a702a_8          19 KB  conda-forge\n",
      "    libgfortran5-11.1.0        |       h6c583b3_8         1.7 MB  conda-forge\n",
      "    libglib-2.68.4             |       h3e27bee_0         3.0 MB  conda-forge\n",
      "    libgomp-11.1.0             |       hc902ee8_8         427 KB  conda-forge\n",
      "    libstdcxx-ng-11.1.0        |       h56837e0_8         4.2 MB  conda-forge\n",
      "    libtiff-4.3.0              |       hf544144_1         668 KB  conda-forge\n",
      "    libwebp-base-1.2.1         |       h7f98852_0         845 KB  conda-forge\n",
      "    lz4-c-1.9.3                |       h9c3ff4c_1         179 KB  conda-forge\n",
      "    mysql-common-8.0.25        |       ha770c72_2         1.6 MB  conda-forge\n",
      "    mysql-libs-8.0.25          |       hfa10184_2         1.8 MB  conda-forge\n",
      "    nss-3.69                   |       hb5efdd6_0         2.1 MB  conda-forge\n",
      "    numba-0.53.1               |   py38h8b71fd7_1         3.7 MB  conda-forge\n",
      "    numpy-1.21.2               |   py38he2449b9_0         6.2 MB  conda-forge\n",
      "    openssl-1.1.1l             |       h7f98852_0         2.1 MB  conda-forge\n",
      "    pcre-8.45                  |       h9c3ff4c_0         253 KB  conda-forge\n",
      "    pynndescent-0.5.4          |     pyh6c4a22f_0          44 KB  conda-forge\n",
      "    python-dateutil-2.8.2      |     pyhd8ed1ab_0         240 KB  conda-forge\n",
      "    python-slugify-5.0.2       |     pyhd8ed1ab_0          12 KB  conda-forge\n",
      "    python_abi-3.8             |           2_cp38           4 KB  conda-forge\n",
      "    pytorch-1.9.0              |cpu_py38h4bbe6ce_2        46.7 MB  conda-forge\n",
      "    readline-8.1               |       h46c0cb4_0         295 KB  conda-forge\n",
      "    scipy-1.7.1                |   py38h56a6a73_0        21.9 MB  conda-forge\n",
      "    setuptools-57.4.0          |   py38h578d9bd_0         934 KB  conda-forge\n",
      "    six-1.16.0                 |     pyh6c4a22f_0          14 KB  conda-forge\n",
      "    sqlalchemy-1.4.23          |   py38h497a2fe_0         2.3 MB  conda-forge\n",
      "    sqlite-3.36.0              |       h9cd32fc_0         1.4 MB  conda-forge\n",
      "    threadpoolctl-2.2.0        |     pyh8a188c0_0          16 KB  conda-forge\n",
      "    tk-8.6.11                  |       h27826a3_1         3.3 MB  conda-forge\n",
      "    tqdm-4.62.2                |     pyhd8ed1ab_0          80 KB  conda-forge\n",
      "    urllib3-1.26.6             |     pyhd8ed1ab_0          99 KB  conda-forge\n",
      "    x264-1!161.3030            |       h7f98852_1         2.5 MB  conda-forge\n",
      "    xorg-libx11-1.7.2          |       h7f98852_0         941 KB  conda-forge\n",
      "    zstd-1.5.0                 |       ha95c52a_0         490 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:       232.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  colorama           conda-forge/noarch::colorama-0.4.4-pyh9f0ad1d_0\n",
      "  jbig               conda-forge/linux-64::jbig-2.1-h7f98852_2003\n",
      "  lerc               conda-forge/linux-64::lerc-2.2.1-h9c3ff4c_0\n",
      "  libdeflate         conda-forge/linux-64::libdeflate-1.7-h7f98852_5\n",
      "\n",
      "The following packages will be REMOVED:\n",
      "\n",
      "  libopenblas-0.3.12-pthreads_h4812303_1\n",
      "  pip-21.0.1-pyhd8ed1ab_0\n",
      "  wheel-0.36.2-pyhd3deb0d_0\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  agate                                          1.6.0-py_3 --> 1.6.1-pyhd8ed1ab_0\n",
      "  agate-dbf                                      0.2.1-py_0 --> 0.2.2-pyhd8ed1ab_0\n",
      "  agate-sql                                      0.5.4-py_0 --> 0.5.7-pyhd8ed1ab_0\n",
      "  babel                                  2.9.0-pyhd3deb0d_0 --> 2.9.1-pyh44b312d_0\n",
      "  cffi                                1.14.5-py38ha65f79e_0 --> 1.14.6-py38ha65f79e_0\n",
      "  expat                                    2.3.0-h9c3ff4c_0 --> 2.4.1-h9c3ff4c_0\n",
      "  ffmpeg                                   4.3.1-hca11adc_2 --> 4.3.2-hca11adc_0\n",
      "  glib                                    2.68.1-h9c3ff4c_0 --> 2.68.4-h9c3ff4c_0\n",
      "  glib-tools                              2.68.1-h9c3ff4c_0 --> 2.68.4-h9c3ff4c_0\n",
      "  greenlet                             1.0.0-py38h709712a_0 --> 1.1.1-py38h709712a_0\n",
      "  gstreamer                               1.18.4-h76c114f_0 --> 1.18.4-h76c114f_2\n",
      "  kiwisolver                           1.3.1-py38h1fd1430_1 --> 1.3.2-py38h1fd1430_0\n",
      "  ld_impl_linux-64                        2.35.1-hea4e1c9_2 --> 2.36.1-hea4e1c9_2\n",
      "  libclang                        11.1.0-default_ha53f305_0 --> 11.1.0-default_ha53f305_1\n",
      "  libgcc-ng                               9.3.0-h2828fa1_19 --> 11.1.0-hc902ee8_8\n",
      "  libgfortran-ng                          9.3.0-hff62375_19 --> 11.1.0-h69a702a_8\n",
      "  libgfortran5                            9.3.0-hff62375_19 --> 11.1.0-h6c583b3_8\n",
      "  libglib                                 2.68.1-h3e27bee_0 --> 2.68.4-h3e27bee_0\n",
      "  libgomp                                 9.3.0-h2828fa1_19 --> 11.1.0-hc902ee8_8\n",
      "  libstdcxx-ng                            9.3.0-h6de172a_19 --> 11.1.0-h56837e0_8\n",
      "  libtiff                                  4.2.0-hdc55705_0 --> 4.3.0-hf544144_1\n",
      "  libwebp-base                             1.2.0-h7f98852_2 --> 1.2.1-h7f98852_0\n",
      "  lz4-c                                    1.9.3-h9c3ff4c_0 --> 1.9.3-h9c3ff4c_1\n",
      "  mysql-common                            8.0.22-ha770c72_3 --> 8.0.25-ha770c72_2\n",
      "  mysql-libs                              8.0.22-h935591d_3 --> 8.0.25-hfa10184_2\n",
      "  nss                                       3.64-hb5efdd6_0 --> 3.69-hb5efdd6_0\n",
      "  numba                               0.53.1-py38h0e12cce_0 --> 0.53.1-py38h8b71fd7_1\n",
      "  numpy                               1.20.2-py38h9894fe3_0 --> 1.21.2-py38he2449b9_0\n",
      "  openssl                                 1.1.1k-h7f98852_0 --> 1.1.1l-h7f98852_0\n",
      "  pcre                                      8.44-he1b5a44_0 --> 8.45-h9c3ff4c_0\n",
      "  pynndescent                            0.5.2-pyh44b312d_0 --> 0.5.4-pyh6c4a22f_0\n",
      "  python-dateutil                                2.8.1-py_0 --> 2.8.2-pyhd8ed1ab_0\n",
      "  python-slugify                         4.0.1-pyh9f0ad1d_0 --> 5.0.2-pyhd8ed1ab_0\n",
      "  python_abi                                     3.8-1_cp38 --> 3.8-2_cp38\n",
      "  pytorch                          1.9.0-cpu_py38hfb3baa6_1 --> 1.9.0-cpu_py38h4bbe6ce_2\n",
      "  readline                                   8.0-he28a2e2_2 --> 8.1-h46c0cb4_0\n",
      "  scipy                                1.6.2-py38h7b17777_0 --> 1.7.1-py38h56a6a73_0\n",
      "  setuptools                          49.6.0-py38h578d9bd_3 --> 57.4.0-py38h578d9bd_0\n",
      "  six                                   1.15.0-pyh9f0ad1d_0 --> 1.16.0-pyh6c4a22f_0\n",
      "  sqlalchemy                           1.4.8-py38h497a2fe_0 --> 1.4.23-py38h497a2fe_0\n",
      "  sqlite                                  3.35.4-h74cdb3f_0 --> 3.36.0-h9cd32fc_0\n",
      "  threadpoolctl                          2.1.0-pyh5ca1d4c_0 --> 2.2.0-pyh8a188c0_0\n",
      "  tk                                      8.6.10-h21135ba_1 --> 8.6.11-h27826a3_1\n",
      "  tqdm                                  4.60.0-pyhd8ed1ab_0 --> 4.62.2-pyhd8ed1ab_0\n",
      "  urllib3                               1.26.4-pyhd8ed1ab_0 --> 1.26.6-pyhd8ed1ab_0\n",
      "  x264                                1!161.3030-h7f98852_0 --> 1!161.3030-h7f98852_1\n",
      "  xorg-libx11                              1.7.0-h7f98852_0 --> 1.7.2-h7f98852_0\n",
      "  zstd                                     1.4.9-ha95c52a_0 --> 1.5.0-ha95c52a_0\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libgomp-11.1.0       | 427 KB    | ##################################### | 100% \n",
      "kiwisolver-1.3.2     | 79 KB     | ##################################### | 100% \n",
      "sqlalchemy-1.4.23    | 2.3 MB    | ##################################### | 100% \n",
      "threadpoolctl-2.2.0  | 16 KB     | ##################################### | 100% \n",
      "libgcc-ng-11.1.0     | 908 KB    | ##################################### | 100% \n",
      "mysql-common-8.0.25  | 1.6 MB    | ##################################### | 100% \n",
      "libstdcxx-ng-11.1.0  | 4.2 MB    | ##################################### | 100% \n",
      "setuptools-57.4.0    | 934 KB    | ##################################### | 100% \n",
      "libgfortran-ng-11.1. | 19 KB     | ##################################### | 100% \n",
      "nss-3.69             | 2.1 MB    | ##################################### | 100% \n",
      "scipy-1.7.1          | 21.9 MB   | ##################################### | 100% \n",
      "libdeflate-1.7       | 67 KB     | ##################################### | 100% \n",
      "mysql-libs-8.0.25    | 1.8 MB    | ##################################### | 100% \n",
      "libglib-2.68.4       | 3.0 MB    | ##################################### | 100% \n",
      "lerc-2.2.1           | 213 KB    | ##################################### | 100% \n",
      "agate-dbf-0.2.2      | 7 KB      | ##################################### | 100% \n",
      "ld_impl_linux-64-2.3 | 667 KB    | ##################################### | 100% \n",
      "urllib3-1.26.6       | 99 KB     | ##################################### | 100% \n",
      "x264-1!161.3030      | 2.5 MB    | ##################################### | 100% \n",
      "expat-2.4.1          | 182 KB    | ##################################### | 100% \n",
      "six-1.16.0           | 14 KB     | ##################################### | 100% \n",
      "zstd-1.5.0           | 490 KB    | ##################################### | 100% \n",
      "libwebp-base-1.2.1   | 845 KB    | ##################################### | 100% \n",
      "libgfortran5-11.1.0  | 1.7 MB    | ##################################### | 100% \n",
      "libtiff-4.3.0        | 668 KB    | ##################################### | 100% \n",
      "babel-2.9.1          | 6.2 MB    | ##################################### | 100% \n",
      "numba-0.53.1         | 3.7 MB    | ##################################### | 100% \n",
      "python-dateutil-2.8. | 240 KB    | ##################################### | 100% \n",
      "lz4-c-1.9.3          | 179 KB    | ##################################### | 100% \n",
      "ffmpeg-4.3.2         | 92.0 MB   | ##################################### | 100% \n",
      "glib-tools-2.68.4    | 86 KB     | ##################################### | 100% \n",
      "tk-8.6.11            | 3.3 MB    | ##################################### | 100% \n",
      "pynndescent-0.5.4    | 44 KB     | ##################################### | 100% \n",
      "sqlite-3.36.0        | 1.4 MB    | ##################################### | 100% \n",
      "numpy-1.21.2         | 6.2 MB    | ##################################### | 100% \n",
      "libclang-11.1.0      | 19.2 MB   | ##################################### | 100% \n",
      "agate-1.6.1          | 54 KB     | ##################################### | 100% \n",
      "python-slugify-5.0.2 | 12 KB     | ##################################### | 100% \n",
      "tqdm-4.62.2          | 80 KB     | ##################################### | 100% \n",
      "pytorch-1.9.0        | 46.7 MB   | ##################################### | 100% \n",
      "agate-sql-0.5.7      | 10 KB     | ##################################### | 100% \n",
      "xorg-libx11-1.7.2    | 941 KB    | ##################################### | 100% \n",
      "openssl-1.1.1l       | 2.1 MB    | ##################################### | 100% \n",
      "jbig-2.1             | 43 KB     | ##################################### | 100% \n",
      "pcre-8.45            | 253 KB    | ##################################### | 100% \n",
      "gstreamer-1.18.4     | 2.0 MB    | ##################################### | 100% \n",
      "greenlet-1.1.1       | 83 KB     | ##################################### | 100% \n",
      "cffi-1.14.6          | 226 KB    | ##################################### | 100% \n",
      "glib-2.68.4          | 447 KB    | ##################################### | 100% \n",
      "readline-8.1         | 295 KB    | ##################################### | 100% \n",
      "python_abi-3.8       | 4 KB      | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda update pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
